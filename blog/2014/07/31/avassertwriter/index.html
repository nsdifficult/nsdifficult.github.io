
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>ä½¿ç”¨AVAssertWriterå½•åˆ¶éŸ³è§†é¢‘åˆ°æ–‡ä»¶ - å›¾æ ·å›¾æ£®ç ´å•ŠğŸ˜„</title>
	<meta name="author" content="æ˜“è£ä¹‰">

	
	<meta name="description" content="ä»Šå¤©å†™ç›´æ’­åŠŸèƒ½ä¸­çš„å½•åˆ¶è§†é¢‘åˆ°æ–‡ä»¶æ—¶ä½¿ç”¨AVAssertWriteré‡åˆ°ä¸€ä¸ªå‘ã€‚ 1
2
3
4
5
6
7 if ([assetWriter canAddInput:assetWriterAudioIn]) { [assetWriter addInput:assetWriterAudioIn]; &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="å›¾æ ·å›¾æ£®ç ´å•ŠğŸ˜„" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script async="true" src="http://libs.useso.com/js/jquery/1.7.2/jquery.min.js"></script>
	
</head>


<body>
	<header id="header" class="inner"><h1><a href="/">å›¾æ ·å›¾æ£®ç ´å•ŠğŸ˜„</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">ä¸»é¡µ</a></li>
	<li><a href="/blog/archives">æ‰€æœ‰æ–‡ç« </a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">ä¸»é¡µ</a></li>
	<li><a href="/blog/archives">æ‰€æœ‰æ–‡ç« </a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="https://www.google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:rongyi.work">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
    
	</div>
	<form class="search" action="https://www.google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:rongyi.work">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner"><article class="post">
	<h2 class="title">ä½¿ç”¨AVAssertWriterå½•åˆ¶éŸ³è§†é¢‘åˆ°æ–‡ä»¶</h2>
	<div class="entry-content"><p>ä»Šå¤©å†™ç›´æ’­åŠŸèƒ½ä¸­çš„å½•åˆ¶è§†é¢‘åˆ°æ–‡ä»¶æ—¶ä½¿ç”¨AVAssertWriteré‡åˆ°ä¸€ä¸ªå‘ã€‚<!--more--></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> if ([assetWriter canAddInput:assetWriterAudioIn]) {
</span><span class='line'>      [assetWriter addInput:assetWriterAudioIn];
</span><span class='line'>        NSLog(@"add asset writer audio input.");
</span><span class='line'>  } else {
</span><span class='line'>        NSLog(@"Couldn't add asset writer audio input.");
</span><span class='line'>        return NO;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>è¿™å¥è¯æ€»æ˜¯æŠ¥<code>Couldn't add asset writer audio input.</code></p>

<p>åæ¥ä¸åˆ¤æ–­ç›´æ¥è°ƒç”¨<code>[assetWriter addInput:assetWriterAudioIn];</code>å‘ç°æŠ¥é”™ï¼š</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[AVAssetWriter addInput:] Cannot call method when status is 1</span></code></pre></td></tr></table></div></figure>


<p>å‘ç°AVAssetWriterçš„statusä¸º1æ—¶ï¼Œæ˜¯ä¸ºAVAssetWriterStatusWritingã€‚å³AVAssetWriteræ­£åœ¨å†™ã€‚</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>enum {
</span><span class='line'>   AVAssetWriterStatusUnknown = 0,
</span><span class='line'>   AVAssetWriterStatusWriting,
</span><span class='line'>   AVAssetWriterStatusCompleted,
</span><span class='line'>   AVAssetWriterStatusFailed,
</span><span class='line'>   AVAssetWriterStatusCancelled
</span><span class='line'>};
</span><span class='line'>typedef NSInteger AVAssetWriterStatus;</span></code></pre></td></tr></table></div></figure>


<p>å°±æ˜¯è¯´AVAssetWriterå·²ç»åœ¨å†™æ–‡ä»¶äº†ï¼Œä¸èƒ½åœ¨å®ƒå†™æ–‡ä»¶çš„æ—¶å€™åŠ å…¥assetWriterAudioInã€‚è¿™ä¸ªé—®é¢˜å¯çœŸå¤Ÿéšè”½çš„ï¼Œè€Œä¸”å®˜æ–¹æ–‡æ¡£æ²¡æœ‰ä»»ä½•è¿™æ–¹é¢çš„æé†’ï¼Œé„™è§†ä¸‹==</p>

<p>è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯ç›´æ¥è°ƒç”¨<code>[assetWriter addInput:assetWriterAudioIn];</code></p>

<p>æœ€åè´´ä¸Šä»£ç ï¼š</p>

<p>é¦–å…ˆåˆå§‹åŒ–ç›¸æœºã€AVCaptureVideoDataOutputã€AVCaptureAudioDataOutputç­‰ï¼š</p>

<pre><code>void  CameraSource::setupCamera(int fps, bool useFront, bool useInterfaceOrientation)
    {
    m_fps = fps;
    m_useInterfaceOrientation = useInterfaceOrientation;


    @autoreleasepool {
        int position = useFront ? AVCaptureDevicePositionFront : AVCaptureDevicePositionBack;

        NSArray* devices = [AVCaptureDevice devices];
        for(AVCaptureDevice* d in devices) {
            if([d hasMediaType:AVMediaTypeVideo] &amp;&amp; [d position] == position)
            {
                m_captureDevice = d;
                NSError* error;
                [d lockForConfiguration:&amp;error];
                [d setActiveVideoMinFrameDuration:CMTimeMake(1, fps)];
                [d setActiveVideoMaxFrameDuration:CMTimeMake(1, fps)];
                [d unlockForConfiguration];
            }
        }

        AVCaptureSession* session = [[AVCaptureSession alloc] init];
        AVCaptureDeviceInput* input;
        AVCaptureVideoDataOutput* output;

        NSString* preset = AVCaptureSessionPresetHigh;
        if(m_usingDeprecatedMethods) {
            int mult = ceil(double(m_targetSize.h) / 270.0) * 270 ;
            switch(mult) {
                case 270:
                    preset = AVCaptureSessionPresetLow;
                    break;
                case 540:
                    preset = AVCaptureSessionPresetMedium;
                    break;
                default:
                    preset = AVCaptureSessionPresetHigh;
                    break;
            }
            session.sessionPreset = preset;
        }
        m_captureSession = session;

        input = [AVCaptureDeviceInput deviceInputWithDevice:((AVCaptureDevice*)m_captureDevice) error:nil];

        output = [[AVCaptureVideoDataOutput alloc] init] ;

        output.videoSettings = @{(NSString*)kCVPixelBufferPixelFormatTypeKey: @(kCVPixelFormatType_32BGRA) };
        if(!m_callbackSession) {
            m_callbackSession = [[sbCallback alloc] init];
        }

        /***éŸ³é¢‘ begin*/
        /*
         * Create audio connection
         */
        NSArray *audioDevices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeAudio];
        AVCaptureDevice *audioDevice;
        if ([audioDevices count] &gt; 0) {
            audioDevice = [audioDevices objectAtIndex:0];
            AVCaptureDeviceInput *audioIn = [[AVCaptureDeviceInput alloc] initWithDevice:audioDevice error:nil];
            if ([session canAddInput:audioIn])
                [session addInput:audioIn];
            [audioIn release];

            AVCaptureAudioDataOutput *audioOut = [[AVCaptureAudioDataOutput alloc] init];
            dispatch_queue_t audioCaptureQueue = dispatch_queue_create("PHONELIVE_AUDIO_QUEUE", DISPATCH_QUEUE_SERIAL);
            [audioOut setSampleBufferDelegate:((sbCallback *)m_callbackSession) queue:audioCaptureQueue];
            dispatch_release(audioCaptureQueue);
            if ([session canAddOutput:audioOut]) {
                [session addOutput:audioOut];
                audioConnection = [audioOut connectionWithMediaType:AVMediaTypeAudio];
            }
            [audioOut release];
        }


        /***éŸ³é¢‘ end*/

        //å½“sampleBufferDelegateå¤„ç†frameæ—¶ï¼Œqueueè¢«blockedï¼Œæ–°æ¥çš„frameä¸å¤„ç†ã€‚ä¸ºYESæ—¶ä¿å­˜ï¼Œä¸blockedæ—¶å¤„ç†ï¼Œè¿™æ ·å ç”¨çš„å†…å­˜å¤šã€‚
        [output setAlwaysDiscardsLateVideoFrames:YES];
        videoDataOutputQueue = dispatch_queue_create("PHONELIVE_VIDEO_QUEUE", DISPATCH_QUEUE_SERIAL);
        //            [output setSampleBufferDelegate:((sbCallback*)m_callbackSession) queue:dispatch_get_global_queue(0, 0)];
        [output setSampleBufferDelegate:((sbCallback*)m_callbackSession) queue:videoDataOutputQueue];
        dispatch_release(videoDataOutputQueue);
        if([session canAddInput:input]) {
            [session addInput:input];
        }
        if([session canAddOutput:output]) {
            [session addOutput:output];
            videoConnection = [output connectionWithMediaType:AVMediaTypeVideo];

        }

        movieWritingQueue = dispatch_queue_create("PHONELIVE_WRITING_QUEUE", DISPATCH_QUEUE_SERIAL);

        reorientCamera();

        [session startRunning];

        if(m_useInterfaceOrientation) {
            [[NSNotificationCenter defaultCenter] addObserver:((id)m_callbackSession) selector:@selector(orientationChanged:) name:UIApplicationDidChangeStatusBarOrientationNotification object:nil];
        } else {
            [[UIDevice currentDevice] beginGeneratingDeviceOrientationNotifications];
            [[NSNotificationCenter defaultCenter] addObserver:((id)m_callbackSession) selector:@selector(orientationChanged:) name:UIDeviceOrientationDidChangeNotification object:nil];
        }
        [output release];

    }
}
</code></pre>

<p>åœ¨didOutputSampleBufferåˆå§‹åŒ–AVAssertWriterå’Œå†™æ–‡ä»¶</p>

<pre><code>- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
   fromConnection:(AVCaptureConnection *)connection
{
auto source = m_source.lock();

if(source) {
    if (connection == source-&gt;videoConnection) {
        source-&gt;bufferCaptured(CMSampleBufferGetImageBuffer(sampleBuffer));
        if (source-&gt;m_isFisrtToUseCamera) {
            source-&gt;m_cameraHasBeingPreparedCallback();
            source-&gt;m_isFisrtToUseCamera = false;
        }
    }

    if (source-&gt;readyToRecordVideo&amp;&amp;source-&gt;readyToRecordAudio) {

        CMFormatDescriptionRef formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer);

        CFRetain(sampleBuffer);
        CFRetain(formatDescription);

        dispatch_async(source-&gt;movieWritingQueue, ^{
            if (connection == source-&gt;videoConnection) {
                if (!source-&gt;m_isRecordingVideo) {
                    // Initialize the video input if this is not done yet
                    source-&gt;m_isRecordingVideo = source-&gt;setupAssetWriterVideoInput(formatDescription);
                }
                if (source-&gt;m_isRecordingVideo &amp;&amp; source-&gt;m_isRecordingAudio) {
                    source-&gt;writeSampleBuffer(sampleBuffer,AVMediaTypeVideo);
                }
            } else if (connection == source-&gt;audioConnection) {
                if (!source-&gt;m_isRecordingAudio) {
                    // Initialize the video input if this is not done yet
                    source-&gt;m_isRecordingAudio = source-&gt;setupAssetWriterAudioInput(formatDescription);
                }
                if (source-&gt;m_isRecordingVideo &amp;&amp; source-&gt;m_isRecordingAudio) {
                    source-&gt;writeSampleBuffer(sampleBuffer,AVMediaTypeAudio);
                }
            }


            CFRelease(sampleBuffer);
            CFRelease(formatDescription);

        });

    }
}
}
</code></pre>

<p>æ³¨æ„ï¼Œè§£å†³æˆ‘é‡åˆ°çš„å‘æ˜¯è¿™å¥:<code>if (source-&gt;m_isRecordingVideo &amp;&amp; source-&gt;m_isRecordingAudio)</code>ï¼Œå³è®¾ç½®ä¸¤ä¸ªå˜é‡æ ‡è¯†AVAssertWriteræ˜¯å¦åŠ å…¥AVCaptureVideoDataOutputå’ŒAVCaptureAudioDataOutputï¼Œåœ¨åŠ å…¥åæ‰å¼€å§‹å†™æ–‡ä»¶ã€‚</p>

<p>æœ€åAVAssertWriteråŠ å…¥AVCaptureVideoDataOutputå’ŒAVCaptureAudioDataOutputã€å†™æ–‡ä»¶çš„æ–¹æ³•ï¼š</p>

<pre><code>BOOL
CameraSource::setupAssetWriterVideoInput(CMFormatDescriptionRef currentFormatDescription)
{
    float bitsPerPixel;
    CMVideoDimensions dimensions = CMVideoFormatDescriptionGetDimensions(currentFormatDescription);
    int numPixels = dimensions.width * dimensions.height;
    int bitsPerSecond;

    // Assume that lower-than-SD resolutions are intended for streaming, and use a lower bitrate
    if ( numPixels &lt; (640 * 480) ) {
        bitsPerPixel = 4.05; // This bitrate matches the quality produced by AVCaptureSessionPresetMedium or Low.
    } else {
        bitsPerPixel = 11.4; // This bitrate matches the quality produced by AVCaptureSessionPresetHigh.

        bitsPerSecond = numPixels * bitsPerPixel;

        NSDictionary *videoCompressionSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                                  AVVideoCodecH264, AVVideoCodecKey,
                                                  [NSNumber numberWithFloat:[UIScreen mainScreen].bounds.size.height*[UIScreen mainScreen].scale], AVVideoWidthKey,
                                                  [NSNumber numberWithFloat:[UIScreen mainScreen].bounds.size.width*[UIScreen mainScreen].scale], AVVideoHeightKey,
                                                  [NSDictionary dictionaryWithObjectsAndKeys:
                                                   [NSNumber numberWithInteger:1000000], AVVideoAverageBitRateKey,
                                                   [NSNumber numberWithInteger:30], AVVideoMaxKeyFrameIntervalKey,
                                                   nil], AVVideoCompressionPropertiesKey,
                                                  nil];

        if ([assetWriter canApplyOutputSettings:videoCompressionSettings forMediaType:AVMediaTypeVideo]) {
            assetWriterVideoIn = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeVideo outputSettings:videoCompressionSettings];
            assetWriterVideoIn.expectsMediaDataInRealTime = YES;
            //                assetWriterVideoIn.transform = [self transformFromCurrentVideoOrientationToOrientation:self.referenceOrientation];
            [assetWriter addInput:assetWriterVideoIn];
            if ([assetWriter canAddInput:assetWriterVideoIn]) {
                [assetWriter addInput:assetWriterVideoIn];
                NSLog(@"add asset writer video input.");
            } else {
                NSLog(@"Couldn't add asset writer video input.");
                return NO;
            }
        } else {
            NSLog(@"Couldn't apply video output settings.");
            return NO;
        }
    }
    return YES;
}

BOOL
CameraSource::setupAssetWriterAudioInput(CMFormatDescriptionRef currentFormatDescription)
{
    const AudioStreamBasicDescription *currentASBD = CMAudioFormatDescriptionGetStreamBasicDescription(currentFormatDescription);

    size_t aclSize = 0;
    const AudioChannelLayout *currentChannelLayout = CMAudioFormatDescriptionGetChannelLayout(currentFormatDescription, &amp;aclSize);
    NSData *currentChannelLayoutData = nil;

    // AVChannelLayoutKey must be specified, but if we don't know any better give an empty data and let AVAssetWriter decide.
    if ( currentChannelLayout &amp;&amp; aclSize &gt; 0 )
        currentChannelLayoutData = [NSData dataWithBytes:currentChannelLayout length:aclSize];
    else
        currentChannelLayoutData = [NSData data];

    NSDictionary *audioCompressionSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                              [NSNumber numberWithInteger:kAudioFormatMPEG4AAC], AVFormatIDKey,
                                              [NSNumber numberWithFloat:currentASBD-&gt;mSampleRate], AVSampleRateKey,
                                              [NSNumber numberWithInt:64000], AVEncoderBitRatePerChannelKey,
                                              [NSNumber numberWithInteger:currentASBD-&gt;mChannelsPerFrame], AVNumberOfChannelsKey,
                                              currentChannelLayoutData, AVChannelLayoutKey,
                                              nil];
    if ([assetWriter canApplyOutputSettings:audioCompressionSettings forMediaType:AVMediaTypeAudio]) {

        assetWriterAudioIn = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeAudio outputSettings:audioCompressionSettings];
        assetWriterAudioIn.expectsMediaDataInRealTime = YES;
        if ([assetWriter canAddInput:assetWriterAudioIn]) {
            [assetWriter addInput:assetWriterAudioIn];
            NSLog(@"add asset writer audio input.");
        } else {
            NSLog(@"Couldn't add asset writer audio input.");
            return NO;
        }
    }
    else {
        NSLog(@"Couldn't apply audio output settings.");
        return NO;
    }

    return YES;
}
void
CameraSource::writeSampleBuffer(CMSampleBufferRef sampleBuffer,NSString *mediaType) {
    if ( assetWriter.status == AVAssetWriterStatusUnknown ) {

        if ([assetWriter startWriting]) {
            CMTime lastSampleTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
            [assetWriter startSessionAtSourceTime:lastSampleTime];
        }
        else {
            //                [self showError:[assetWriter error]];
            NSLog(@"%@",assetWriter.error);

        }
    }

    if ( assetWriter.status == AVAssetWriterStatusWriting ) {

        if (mediaType == AVMediaTypeVideo) {
            if (assetWriterVideoIn.readyForMoreMediaData) {
                if (![assetWriterVideoIn appendSampleBuffer:sampleBuffer]) {
                    //                        [self showError:[assetWriter error]];
                    NSLog(@"%@",assetWriter.error);
                }
            }
        }else if (mediaType == AVMediaTypeAudio) {
            if (assetWriterAudioIn.readyForMoreMediaData) {
                if (![assetWriterAudioIn appendSampleBuffer:sampleBuffer]) {
                    //                        [self showError:[assetWriter error]];
                    NSLog(@"%@",assetWriter.error);

                }
            }
        }
    }
}
</code></pre>

<p>å‚è€ƒï¼š<br/>
1. <a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/05_Export.html#//apple_ref/doc/uid/TP40010188-CH9-SW2">AV Foundation Programming Guide</a><br/>
2. <a href="https://developer.apple.com/library/ios/samplecode/RosyWriter/Introduction/Intro.html">å®˜æ–¹demoï¼šRosyWriter</a></p>
</div>


<div class="meta">
	<div class="date">




Jul 31st, 2014</div>
	<div class="tags">

</div>
	
</div>
</article>

	<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
		
		
		
		
	</div>
	
</div>


</div>
	<footer id="footer" class="inner">Copyright &copy; 2015

    æ˜“è£ä¹‰

</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->






</body>
</html>